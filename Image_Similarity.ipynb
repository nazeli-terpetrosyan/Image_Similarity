{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Similarity.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "UCZn7GzevW15"
      ],
      "authorship_tag": "ABX9TyPpiC3VuTKClFuycPNiSDQA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazeli-terpetrosyan/Image_Similarity/blob/main/Image_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Determining relevant supplier websites based on the product images"
      ],
      "metadata": {
        "id": "r3WpCNgsuPYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this task, I took 2 approaches: measuring the image similarity with cosine distance and building a Siamese Neural Network(SNN)."
      ],
      "metadata": {
        "id": "b2jkQrZquc9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#API Research"
      ],
      "metadata": {
        "id": "UCZn7GzevW15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before writing the code, I did some research regarding other projects done for image similarity. Below, I'll present some APIs that could be useful for solving the task."
      ],
      "metadata": {
        "id": "I0UG5uPxu6vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep AI Image Similarity API**"
      ],
      "metadata": {
        "id": "GYuBXrlOOq3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep AI's API had a significantly good performance, however, to use the API in production, there would have been a need to discuss partnership and maybe further contracts with Deep API, therefore, it would not have been the best solution.\n",
        "\n",
        "The [link ](https://deepai.org/machine-learning-model/image-similarity)to the API webpage."
      ],
      "metadata": {
        "id": "wNRU1q1Mv2Y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image similarity API published on Rapid API**"
      ],
      "metadata": {
        "id": "biCWSVfnwQCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This API also had a good performance, however, it had limitations for free use. You could only make 20 calls per day. Their Pro plan allowed 6000 calls/month, costing 1.99$, however, it would not have been useful in the long run. \n",
        "\n",
        "The [link ](https://rapidapi.com/dyapi-dyapi-default/api/image-similarity1)to the API webpage."
      ],
      "metadata": {
        "id": "lbKlXGmLwYTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Therefore,** after finalizing the initial research and not finding any appropriate APIs, I started working on the code."
      ],
      "metadata": {
        "id": "VB5SGiKsxtpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing the Libraries"
      ],
      "metadata": {
        "id": "hE4IW2eg2j5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "emBFy1Av2nE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Approach 1: Cosine Distance"
      ],
      "metadata": {
        "id": "EO3jNO43OZ9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "metric = 'cosine' #the method used for calculating the distance (other options are euclidean or dot for example)"
      ],
      "metadata": {
        "id": "bJVnXbPEolxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = \"https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2\"\n",
        "\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "\n",
        "layer = hub.KerasLayer(model_url)\n",
        "emb_model = tf.keras.Sequential([layer])"
      ],
      "metadata": {
        "id": "u9KqnLx9ogOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(file):\n",
        "  file = Image.open(file).convert('L').resize(IMAGE_SHAPE)\n",
        "\n",
        "  file = np.stack((file,)*3, axis=-1)\n",
        "\n",
        "  file = np.array(file)/255.0\n",
        "\n",
        "  embedding = emb_model.predict(file[np.newaxis, ...])\n",
        "\n",
        "  feature_np = np.array(embedding)\n",
        "  flattended_feature = feature_np.flatten()\n",
        "\n",
        "  return flattended_feature"
      ],
      "metadata": {
        "id": "GrOCOkN8ohwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample code for cosine distance"
      ],
      "metadata": {
        "id": "gPcwwFqE4ROP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "img1 = extract('{IMG1 PATH HERE}')\n",
        "img2 = extract('{IMG2 PATH HERE}')\n",
        "distance.cdist([img1], [img2], metric)[0]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PX1OWs7THkok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is between 0 and 1. 0 means the 2 images are exactly the same. Therefore, the larger the output the more different the images are."
      ],
      "metadata": {
        "id": "8bvyDwEK4fS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The disadvantage** of cosine similarity is that for the algorithm almonds against a white background and hazelnuts against a white background are 2 very similiar images. In this case, it can be disadvantegous when checking for product suppliers.\n",
        "\n",
        "*A solution might be trying to fine-tune the embedding model for better feature extraction. And another approach can be trying to play around with different metric systems."
      ],
      "metadata": {
        "id": "snl6D-h642L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Approach 2: Siamese Neural Network"
      ],
      "metadata": {
        "id": "Ji91lSM8ONms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting the Dataset"
      ],
      "metadata": {
        "id": "lciGLjjmZfLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rioxl0DyZqDA",
        "outputId": "a5ef2eec-3e81-4bb5-9e8b-caa85ab7096f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhKCe9NDZV86"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "PATH = '/content/drive/MyDrive/google_images.zip' #Path to the images zip\n",
        "\n",
        "zip_ref = zipfile.ZipFile(PATH, 'r')\n",
        "zip_ref.extractall('/tmp/train')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datadir='/tmp/train'\n",
        "Categories=os.listdir(datadir)\n",
        "\n",
        "img_data=[]\n",
        "label=[]\n",
        "SIZE = 128 #The size of the images\n",
        "\n",
        "for i in Categories:\n",
        "  print(f'loading... category : {i}')\n",
        "  path=os.path.join(datadir, i)\n",
        "  for img in os.listdir(path):\n",
        "    img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_COLOR)     \n",
        "    img_array = cv2.resize(img_array, (SIZE, SIZE))  \n",
        "    img_data.append(img_array)\n",
        "    label.append(Categories.index(i))\n",
        "  print(f'loaded category: {i} successfully')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMMAWLPvc6_H",
        "outputId": "7c1b23ab-6496-4a1e-f8a0-4718d72870b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading... category : peanuts\n",
            "loaded category: peanuts successfully\n",
            "loading... category : coconut\n",
            "loaded category: coconut successfully\n",
            "loading... category : sunflower oil\n",
            "loaded category: sunflower oil successfully\n",
            "loading... category : oats\n",
            "loaded category: oats successfully\n",
            "loading... category : hazelnuts\n",
            "loaded category: hazelnuts successfully\n",
            "loading... category : random\n",
            "loaded category: random successfully\n",
            "loading... category : maple syrup\n",
            "loaded category: maple syrup successfully\n",
            "loading... category : cocoa\n",
            "loaded category: cocoa successfully\n",
            "loading... category : dates\n",
            "loaded category: dates successfully\n",
            "loading... category : almonds\n",
            "loaded category: almonds successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating Image Pairs"
      ],
      "metadata": {
        "id": "wAXDbb7VaLoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function generates images pairs solely compared with the target(in our case, it will be almonds)."
      ],
      "metadata": {
        "id": "RHYZ-t8_9y4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pairs_target(images, labels, target):\n",
        "  pairImages = []\n",
        "  pairLabels = []\n",
        "\n",
        "\n",
        "  numClasses = len(np.unique(labels))\n",
        "  idx = [np.where(labels == Categories[i])[0] for i in range(0, numClasses)]\n",
        "\n",
        "  target_imgs = images[np.where(labels == target)]\n",
        "\n",
        "  for img in target_imgs:\n",
        "\t\t# randomly pick an image that belongs to the same class (creating a positive pair)\n",
        "    idxB = np.random.choice(idx[Categories.index(target)])\n",
        "    posImage = images[idxB]\n",
        "    pairImages.append([img, posImage])\n",
        "    pairLabels.append([1])\n",
        "\n",
        "    # randomly pick an image that belongs to a different class (creating a negative pair)\n",
        "    negIdx = np.where(labels != target)[0]\n",
        "    negImage = images[np.random.choice(negIdx)]\n",
        "    pairImages.append([img, negImage])\n",
        "    pairLabels.append([0])\n",
        "\n",
        "  return (np.array(pairImages), np.array(pairLabels))"
      ],
      "metadata": {
        "id": "gelYxZ-6eNis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function generates image pairs with combinations of all categories."
      ],
      "metadata": {
        "id": "DJ155F_m-vFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pairs(images, labels):\n",
        "  pairImages = []\n",
        "  pairLabels = []\n",
        "\n",
        "  numClasses = len(np.unique(labels))\n",
        "  idx = [np.where(labels == i)[0] for i in range(0, numClasses)]\n",
        "\n",
        "  for idxA in range(len(images)):\n",
        "    currentImage = images[idxA]\n",
        "    currentLabel = labels[idxA]\n",
        "\n",
        "\t\t# randomly pick an image that belongs to the same class (creating a positive pair)\n",
        "    idxB = np.random.choice(idx[currentLabel])\n",
        "    posImage = images[idxB]\n",
        "    pairImages.append([currentImage, posImage])\n",
        "    pairLabels.append([1])\n",
        "\n",
        "\t\t# randomly pick an image that belongs to a different class (creating a negative pair)\n",
        "    negIdx = np.where(labels != label)[0]\n",
        "    negImage = images[np.random.choice(negIdx)]\n",
        "    pairImages.append([currentImage, negImage])\n",
        "    pairLabels.append([0])\n",
        "\t\n",
        "  return (np.array(pairImages), np.array(pairLabels))"
      ],
      "metadata": {
        "id": "hGaWdKNL1aLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "kPI_7rjd-3MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(img_data, label, test_size = 0.2, random_state = 1)\n",
        "\n",
        "x_train, x_test = np.array(x_train) / 255.0, np.array(x_test) / 255.0\n",
        "\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "(pairTrain, labelTrain) = make_pairs(np.array(x_train), np.array(y_train))\n",
        "(pairTest, labelTest) = make_pairs(np.array(x_test), np.array(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3co4EUCtejZw",
        "outputId": "641f88cd-1d13-4367-beed-1aee4ad88302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first, the data was generated with the *make_pairs_target()* function, however due to insufficient data, the model was later trained with the data generated from *make_pairs()* function."
      ],
      "metadata": {
        "id": "LkhmelCJ_ImO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SNN Architecture"
      ],
      "metadata": {
        "id": "_SKwovMInfxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "import tensorflow.keras.backend as K"
      ],
      "metadata": {
        "id": "F167sS2J3QNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Architecture #1"
      ],
      "metadata": {
        "id": "lMvZFzjtAX6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE = (128, 128, 1)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100"
      ],
      "metadata": {
        "id": "j-0TYwYqpdYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_siamese_model(inputShape, embeddingDim=48):\n",
        "\tinputs = Input(inputShape)\n",
        "\n",
        "\tx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(inputs)\n",
        "\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\tx = Dropout(0.3)(x)\n",
        "\tx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "\tx = MaxPooling2D(pool_size=2)(x)\n",
        "\tx = Dropout(0.3)(x)\n",
        " \n",
        "\tpooledOutput = GlobalAveragePooling2D()(x)\n",
        "\toutputs = Dense(embeddingDim)(pooledOutput)\n",
        "\n",
        "\tmodel = Model(inputs, outputs)\n",
        "\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "0fZfwshfqAWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(vectors):\n",
        "\t(featsA, featsB) = vectors\n",
        "\n",
        "\tsumSquared = K.sum(K.square(featsA - featsB), axis=1,\n",
        "\t\tkeepdims=True)\n",
        "\n",
        "\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))"
      ],
      "metadata": {
        "id": "NzLr3bSeqcM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgA = Input(shape=IMG_SHAPE)\n",
        "imgB = Input(shape=IMG_SHAPE)\n",
        "featureExtractor = build_siamese_model(IMG_SHAPE)\n",
        "featsA = featureExtractor(imgA)\n",
        "featsB = featureExtractor(imgB)"
      ],
      "metadata": {
        "id": "T6CGn2iRsu0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist = Lambda(euclidean_distance)([featsA, featsB])\n",
        "outputs = Dense(1, activation=\"sigmoid\")(dist)\n",
        "model = Model(inputs=[imgA, imgB], outputs=outputs)"
      ],
      "metadata": {
        "id": "g_vblFbWs5yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(\n",
        "\t[pairTrain[:, 0][:, :, :, 0], pairTrain[:, 1][:, :, :, 0]], labelTrain[:],\n",
        "\tvalidation_data=([pairTest[:, 0][:, :, :, 0], pairTest[:, 1][:, :, :, 0]], labelTest[:]),\n",
        "\tbatch_size=BATCH_SIZE, \n",
        "  epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "IC8MYdi8s-Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Architecture #2"
      ],
      "metadata": {
        "id": "djzRyPGcAaSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input((128, 128, 1))\n",
        "\n",
        "x = BatchNormalization()(input)\n",
        "x = Conv2D(16, (2, 2), activation=\"tanh\")(x)\n",
        "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(32, (2, 2), activation=\"tanh\")(x)\n",
        "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(64, (2, 2), activation=\"tanh\")(x)\n",
        "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(128, (2, 2), activation=\"tanh\")(x)\n",
        "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(256, (2, 2), activation=\"tanh\")(x)\n",
        "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(10, activation=\"tanh\", kernel_regularizer=\"l2\")(x)\n",
        "\n",
        "embedding_network = Model(input, x)\n",
        "\n",
        "input_1 = Input((128, 128, 1))\n",
        "input_2 = Input((128, 128, 1))\n",
        "\n",
        "tower_1 = embedding_network(input_1)\n",
        "tower_2 = embedding_network(input_2)\n",
        "\n",
        "merge_layer = Lambda(euclidean_distance)([tower_1, tower_2])\n",
        "normal_layer = BatchNormalization()(merge_layer)\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(normal_layer)\n",
        "\n",
        "siamese = Model(inputs=[input_1, input_2], outputs=output_layer)"
      ],
      "metadata": {
        "id": "IFFky83h4m9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "history = siamese.fit(\n",
        "\t[pairTrain[:, 0][:, :, :, 0], pairTrain[:, 1][:, :, :, 0]], labelTrain[:],\n",
        "\tvalidation_data=([pairTest[:, 0][:, :, :, 0], pairTest[:, 1][:, :, :, 0]], labelTest[:]),\n",
        "\tbatch_size=BATCH_SIZE, \n",
        "  epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "6dllfQ_r48nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two results showed after training the both models with different losses and activation functions, Ðµither the model was not learning, or it was overfitting. These 2 results were a consequence of insufficient data."
      ],
      "metadata": {
        "id": "cZuZbcgbBIhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This showed that for measuring similarity for a specific product (almonds in this case), SNNs are not the best solution. However, if there is no restriction to a specific product and more data available, I believe it is highly possible SNN's will perform better than other methods (in this case, cosine distance)."
      ],
      "metadata": {
        "id": "n1qV-Fu7BhEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TensorFlow Image Similarity"
      ],
      "metadata": {
        "id": "V9ZdroLBypG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow recently introduced the TensorFlow Similarity package to make training image similarity models easier. Below, you can see the demo version on the MNIST dataset.\n",
        "\n",
        "The model below returns the 5 closest images to the given image. This is not exactly what this task requires, but I believe the model can be changed to solve this task.\n",
        "\n",
        "*Due to limited time and resources regarding the package, I was not able to review it thoroughly. However, I believe TensorFlow Similarity can be quite useful for solving the task in the future."
      ],
      "metadata": {
        "id": "dGZmsISNEqO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade-strategy=only-if-needed tensorflow_similarity[tensorflow] "
      ],
      "metadata": {
        "id": "D4x048sly0zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Embedding output layer with L2 norm\n",
        "from tensorflow_similarity.layers import MetricEmbedding \n",
        "# Specialized metric loss\n",
        "from tensorflow_similarity.losses import MultiSimilarityLoss \n",
        "# Sub classed keras Model with support for indexing\n",
        "from tensorflow_similarity.models import SimilarityModel\n",
        "# Data sampler that pulls datasets directly from tf dataset catalog\n",
        "from tensorflow_similarity.samplers import TFDatasetMultiShotMemorySampler\n",
        "# Nearest neighbor visualizer\n",
        "from tensorflow_similarity.visualization import viz_neigbors_imgs\n",
        "\n",
        "\n",
        "# Data sampler that generates balanced batches from MNIST dataset\n",
        "sampler = TFDatasetMultiShotMemorySampler(dataset_name='mnist', classes_per_batch=10)\n",
        "\n",
        "# Build a Similarity model using standard Keras layers\n",
        "inputs = layers.Input(shape=(28, 28, 1))\n",
        "x = layers.Rescaling(1/255)(inputs)\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "outputs = MetricEmbedding(64)(x)\n",
        "\n",
        "# Build a specialized Similarity model\n",
        "model = SimilarityModel(inputs, outputs)\n",
        "\n",
        "# Train Similarity model using contrastive loss\n",
        "model.compile('adam', loss=MultiSimilarityLoss())\n",
        "model.summary()\n",
        "model.fit(sampler, epochs=5)\n",
        "\n",
        "# Index 100 embedded MNIST examples to make them searchable\n",
        "sx, sy = sampler.get_slice(0,100)\n",
        "model.index(x=sx, y=sy, data=sx)\n",
        "\n",
        "# Find the top 5 most similar indexed MNIST examples for a given example\n",
        "qx, qy = sampler.get_slice(3713, 1)\n",
        "nns = model.single_lookup(qx[0])\n",
        "\n",
        "# Visualize the query example and its top 5 neighbors\n",
        "#viz_neigbors_imgs(qx[0], qy[0], nns)"
      ],
      "metadata": {
        "id": "nd_CrBx8x6Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting website relevance"
      ],
      "metadata": {
        "id": "1Nsayxa48_LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A description of the approach:** The code goes over the images scrapped from the website and compares them with the images of the target(in this case, almonds). Every image is compared n times (VAL_NUM) with images randomly chosen from the target images folder, and then the mean is taken. Based on the mean, it is determined if the image is similar or not (according to SIM_THRESHOLD). Then the percentage of similar images on the website is determined, which then determines the website's relevance (according to RELEVANCE_THRESHOLD)."
      ],
      "metadata": {
        "id": "THRmwIARMDb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datadir='/content/drive/MyDrive/Data/Irrelevant/Example 1' #the path to the scrapped website images\n",
        "\n",
        "TARGET = 'almonds' #the target can be easily modified to check for other products\n",
        "validation_datadir='/tmp/train/' + TARGET #the path to the folder where target's images are located(for comparison)\n",
        "VAL_NUM = 15 #the number of images that the website image will be compared with\n",
        "\n",
        "#The threshold that will be considered for similarity\n",
        "#In this case, it's 0.65 since the data for validation might contain not relevant images and therefore throw off the mean\n",
        "#when you're sure that the validation data is clean and reliable, you can lower the threshold\n",
        "SIM_THRESHOLD = 0.65 \n",
        "\n",
        "#The percentage of images on the website that were similar to the target\n",
        "#Currently, it's at 0.25 since I believe that if the 1/4 of websites images are relevant, \n",
        "#then the website has a large chance of being a potential supplier\n",
        "RELEVANT_THRESHOLD = 0.25\n",
        "\n",
        "imgs = os.listdir(validation_datadir)\n",
        "label = np.array(label)\n",
        "target_idx = np.where(label == Categories.index(TARGET))[0]\n",
        "\n",
        "sim_arr = []\n",
        "\n",
        "for img in os.listdir(datadir):\n",
        "  img1 = extract(os.path.join(datadir,img))\n",
        "  img_mean = 0 \n",
        "  for i in range(VAL_NUM): \n",
        "    #The 2nd image is choosen randomly from the validation folder, however specific images can be used too\n",
        "    img2 = extract(os.path.join(validation_datadir, np.random.choice(imgs)))\n",
        "    #Here cosine distance is used, however the comparison method can easily be changed\n",
        "    img_mean += distance.cdist([img1], [img2], metric)[0] \n",
        "  img_mean /= VAL_NUM\n",
        "  sim_arr.append(1 if img_mean <= SIM_THRESHOLD else 0)\n",
        "\n",
        "sim_arr = np.array(sim_arr)\n",
        "prct = len(sim_arr[sim_arr == 1])/len(sim_arr)\n",
        "print(\"Relevant\" if prct >= RELEVANT_THRESHOLD else \"Irrelevant\")\n",
        "print(\"Percentage: \" + str(prct))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-wIN5OYrq9Z",
        "outputId": "518b3b1b-7c9f-4162-f6ec-4aa7d1e597d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Irrelevant\n",
            "Percentage: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some Results**\n",
        "\n",
        "*Relevant Example 1:* 0.47 (Relevant)\n",
        "\n",
        "*Relevant Example 2:* 0.45454545454545453 (Relevant)\n",
        "\n",
        "*Relevant Example 3:* 0.25 (Relevant)\n",
        "\n",
        "\\\n",
        "\n",
        "*Irrelevant Example 1:* 0.0 (Irrelevant)\n",
        "\n",
        "*Irrelevant Example 2:* 0.02857142857142857 (Irrelevant)\n",
        "\n",
        "*Irrelevant Example 3:* 0.06172839506172839 (Irrelevant)"
      ],
      "metadata": {
        "id": "ZbrSFtnmI1tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks:** Currently, the algorithm of the website relevance is quite slow as it needs to do a significant number of validations for better accuracy. However, as the image similarity model's accuracy increases, there will be less need for numerous validations, reducing the execution time."
      ],
      "metadata": {
        "id": "4p2np3yrLQ1F"
      }
    }
  ]
}